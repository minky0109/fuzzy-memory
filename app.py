import streamlit as st
import fitz  # PyMuPDF
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 1. í˜ì´ì§€ ì„¤ì • ë° ë””ìì¸
st.set_page_config(page_title="ë¬¸í•­ ìœ ì‚¬ë„ ë¶„ì„ê¸°", layout="wide")
st.markdown("""
    <style>
    .stApp { background-color: #FFF5F7; }
    h1, h2, h3 { color: #D63384; }
    div.stButton > button {
        width: 100%; background-color: #FFB6C1; color: white;
        border-radius: 12px; border: none; height: 3.5em; font-weight: bold;
    }
    .compare-box {
        border: 2px solid #FFB6C1; padding: 20px; border-radius: 15px;
        background-color: white; color: black; min-height: 250px; line-height: 1.8;
        font-size: 1.05rem;
    }
    mark { background-color: #FFD1DC; color: #D63384; font-weight: bold; border-radius: 3px; padding: 0 1px; }
    </style>
    """, unsafe_allow_html=True)

# --- [ê°œì„ ] ë…¸ì´ì¦ˆ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ê·œí™” ---
def clean_text(text):
    # ì¤„ë°”ê¿ˆ, íƒ­, ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ í†µì¼
    text = re.sub(r'\s+', ' ', text)
    # íŠ¹ìˆ˜ ê¸°í˜¸ ì •ê·œí™” (ë¹„êµ ì •í™•ë„ í–¥ìƒ)
    text = text.replace('â€œ', '"').replace('â€', '"').replace('â€˜', "'").replace('â€™', "'")
    return text.strip()

def extract_problems_with_pages(file):
    if file is None: return []
    file.seek(0)
    doc = fitz.open(stream=file.read(), filetype="pdf")
    all_problems = []
    
    noise_words = ['í•™ë…„ë„', 'ì˜ì—­', 'í™•ì¸ì‚¬í•­', 'ìœ ì˜ì‚¬í•­', 'ì„±ëª…', 'ìˆ˜í—˜ë²ˆí˜¸', 'ìƒí™œê³¼ ìœ¤ë¦¬']

    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        page_text = page.get_text("text")
        
        # ë¬¸í•­ ë²ˆí˜¸ íŒ¨í„´ ìª¼ê°œê¸°
        split_text = re.split(r'\n(?=\d+[\.|\)])|(?<=\n)(?=\d+[\.|\)])|(?=\[\d+\])', page_text)
        
        for p in split_text:
            cleaned_p = clean_text(p)
            
            # [í•„í„°] ìˆ«ìë¡œ ì‹œì‘ ì•ˆí•˜ê±°ë‚˜ ë„ˆë¬´ ì§§ê±°ë‚˜ ë…¸ì´ì¦ˆ ë‹¨ì–´ê°€ ì•ë¶€ë¶„ì— ìˆìœ¼ë©´ í†µê³¼
            if not re.match(r'^(\d+|\[\d+|[â‘ -â‘³])', cleaned_p) or len(cleaned_p) < 45:
                continue
            if any(nw in cleaned_p[:30] for nw in noise_words):
                continue

            all_problems.append({"text": cleaned_p, "page": page_num + 1})
    return all_problems

# --- [í•µì‹¬ ë³´ì™„] ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ ---
def highlight_precise(target, reference):
    """
    ë‹¨ì–´ ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ 4ê¸€ì ì´ìƒì˜ ê³µí†µ ë¬¸ìì—´ì„ ì°¾ì•„ í•˜ì´ë¼ì´íŠ¸í•©ë‹ˆë‹¤.
    ì¡°ì‚¬ë‚˜ ì–´ë¯¸ê°€ ë‹¬ë¼ë„ í•µì‹¬ ë¬¸êµ¬ëŠ” ëª¨ë‘ ì¡ì•„ëƒ…ë‹ˆë‹¤.
    """
    # ë¹„êµë¥¼ ìœ„í•´ ê³µë°± ì œê±° ë²„ì „ ìƒì„±
    ref_stripped = re.sub(r'\s+', '', reference)
    
    # ê³µë°±ì„ í¬í•¨í•œ ì›ë¬¸ì—ì„œ 4ê¸€ì ì´ìƒì˜ ê³µí†µ ë¶€ë¶„ ì°¾ê¸°
    # ìµœì†Œ 4ê¸€ì ì—°ì† ì¼ì¹˜ ì‹œ í•˜ì´ë¼ì´íŠ¸ ëŒ€ìƒ
    min_len = 4
    to_highlight = set()
    
    # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ì—ì„œ ìœˆë„ìš°ë¥¼ ë°€ë©´ì„œ ì°¸ì¡° í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    words = target.split()
    for i in range(len(target) - min_len + 1):
        chunk = target[i:i+min_len]
        if chunk.strip() == "": continue
        
        # ê³µë°± ì œê±°í•˜ê³  ë¹„êµ (ì¡°ì‚¬ ì°¨ì´ ê·¹ë³µ)
        chunk_stripped = re.sub(r'\s+', '', chunk)
        if chunk_stripped in ref_stripped and len(chunk_stripped) >= 3:
            to_highlight.add(chunk)

    # í•˜ì´ë¼ì´íŠ¸í•  ë¬¸êµ¬ë“¤ì„ ê¸¸ì´ ìˆœ(ê¸´ ê²ƒë¶€í„°) ì •ë ¬
    sorted_chunks = sorted(list(to_highlight), key=len, reverse=True)
    
    result = target
    for chunk in sorted_chunks:
        # ì¤‘ë³µ í•˜ì´ë¼ì´íŠ¸ ë°©ì§€ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ì¹˜í™˜ ì‚¬ìš©
        if chunk in result:
            result = result.replace(chunk, f"<mark>{chunk}</mark>")
    
    # ì¤‘ì²©ëœ mark íƒœê·¸ ì •ë¦¬ (ì •ê·œì‹ ì‚¬ìš©)
    result = re.sub(r'</mark><mark>', '', result)
    return result

# --- UI ë° ë¶„ì„ ë¡œì§ ---
st.title("ğŸ” ë¬¸í•­ ìœ ì‚¬ë„ ì •ë°€ ë¶„ì„ê¸° (ì •í™•ë„ ê°•í™”)")

col1, col2 = st.columns(2)
with col1:
    file_origin = st.file_uploader("ğŸ“˜ ê¸°ì¤€ PDF", type="pdf")
with col2:
    file_new = st.file_uploader("ğŸ“ ëŒ€ìƒ PDF", type="pdf")

if file_origin and file_new:
    if st.button("âœ¨ ë¶„ì„ ì‹œì‘í•˜ê¸°"):
        with st.spinner('ë¬¸ì ë‹¨ìœ„ë¡œ ì •ë°€ ëŒ€ì¡° ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):
            list_origin = extract_problems_with_pages(file_origin)
            list_new = extract_problems_with_pages(file_new)
            
            if not list_origin or not list_new:
                st.error("ë¬¸í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                results = []
                # ìœ ì‚¬ë„ ë¶„ì„ì€ ë¬¸ì¥ íë¦„(n-gram) ë°˜ì˜
                vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char') 
                
                for i, new_item in enumerate(list_new):
                    new_p = new_item['text']
                    best_score, best_match, found_page = 0, "", 0
                    
                    for origin_item in list_origin:
                        origin_p = origin_item['text']
                        try:
                            tfidf = vectorizer.fit_transform([new_p, origin_p])
                            score = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]
                            if score > best_score:
                                best_score, best_match, found_page = score, origin_p, origin_item['page']
                        except: continue
                    
                    results.append({
                        "id": i + 1, "score": round(best_score * 100, 1),
                        "origin": best_match, "new": new_p, "page": found_page
                    })
                st.session_state.results = results

if 'results' in st.session_state:
    st.markdown("---")
    for res in st.session_state.results:
        status = "âœ…"
        if res['score'] > 40:
            status = "ğŸš¨ ìœ„í—˜" if res['score'] > 70 else "âš ï¸ ì£¼ì˜"
        
        label = f"{status} | {res['id']}ë²ˆ (ìœ ì‚¬ë„ {res['score']}%)[ì›ë³¸ {res['page']}p]"
        with st.expander(label):
            # ì •ë°€ í•˜ì´ë¼ì´íŠ¸ ì ìš©
            h_new = highlight_precise(res['new'], res['origin'])
            h_origin = highlight_precise(res['origin'], res['new'])
            
            c1, c2 = st.columns(2)
            with c1: st.markdown(f"<div class='compare-box'><b>[ì¶œì œ ë¬¸í•­]</b><hr>{h_new}</div>", unsafe_allow_html=True)
            with c2: st.markdown(f"<div class='compare-box'><b>[ê¸°ì¤€ ë¬¸í•­ - {res['page']}p]</b><hr>{h_origin}</div>", unsafe_allow_html=True)
