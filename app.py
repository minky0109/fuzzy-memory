import streamlit as st
import fitz
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 1. í˜ì´ì§€ ì„¤ì • ë° ë””ìì¸
st.set_page_config(page_title="ë¬¸í•­ ìœ ì‚¬ë„ ë¶„ì„ê¸°", layout="wide")
st.markdown("""
    <style>
    .stApp { background-color: #FFF5F7; }
    h1, h2, h3 { color: #D63384; }
    div.stButton > button {
        width: 100%; background-color: #FFB6C1; color: white;
        border-radius: 12px; border: none; height: 3.5em; font-weight: bold;
    }
    .compare-box {
        border: 2px solid #FFB6C1; padding: 20px; border-radius: 15px;
        background-color: white; color: black; min-height: 250px; line-height: 1.8;
    }
    /* í•˜ì´ë¼ì´íŠ¸ ìƒ‰ìƒì„ ì¡°ê¸ˆ ë” ì„ ëª…í•˜ê²Œ, ê¸€ììƒ‰ì€ ê²€ì • ìœ ì§€ */
    mark { background-color: #FFC0CB; color: black; font-weight: bold; border-radius: 3px; padding: 0 2px; }
    </style>
    """, unsafe_allow_html=True)

# --- [ê°œì„ ] í…ìŠ¤íŠ¸ ì •ê·œí™” (ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°) ---
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_problems_with_pages(file):
    if file is None: return []
    file.seek(0)
    doc = fitz.open(stream=file.read(), filetype="pdf")
    all_problems = []
    # í™•ì‹¤íˆ ê±¸ëŸ¬ì•¼ í•  ë…¸ì´ì¦ˆ íŒ¨í„´
    noise_keywords = ['í•™ë…„ë„', 'ì˜ì—­', 'í™•ì¸ì‚¬í•­', 'ìœ ì˜ì‚¬í•­', 'ì„±ëª…', 'ìˆ˜í—˜ë²ˆí˜¸']

    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        page_text = page.get_text()
        split_text = re.split(r'\n(?=\d+[\.|\)])|(?<=\n)(?=\d+[\.|\)])|(?=\[\d+\])', page_text)
        
        for p in split_text:
            cleaned_p = clean_text(p)
            # ìˆ«ìë¡œ ì‹œì‘í•˜ê³  ì¼ì • ê¸¸ì´ ì´ìƒì¸ ê²ƒë§Œ
            if re.match(r'^(\d+|\[\d+|[â‘ -â‘³])', cleaned_p) and len(cleaned_p) >= 50:
                if not any(nk in cleaned_p[:30] for nk in noise_keywords):
                    all_problems.append({"text": cleaned_p, "page": page_num + 1})
    return all_problems

# --- [ê°œì„ ] ë³€ë³„ë ¥ ìˆëŠ” í•˜ì´ë¼ì´íŠ¸ (ìµœì†Œ 6ê¸€ì ì¼ì¹˜ ì‹œì—ë§Œ) ---
def highlight_selective(target, reference):
    """
    í”í•œ ë‹¨ì–´ëŠ” ë¬´ì‹œí•˜ê³ , 6ê¸€ì ì´ìƒì˜ ê³ ìœ í•œ ë¬¸êµ¬ê°€ ê²¹ì¹  ë•Œë§Œ í•˜ì´ë¼ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    ref_stripped = re.sub(r'\s+', '', reference)
    # ì˜ë¯¸ ì—†ëŠ” ì§§ì€ ì—°ê²°ì–´ë“¤ (ì¡°ì‚¬, ì ‘ì†ì‚¬ ë“± ë°©ì§€)
    # ìµœì†Œ ê¸¸ì´ë¥¼ 6ê¸€ìë¡œ ìƒí–¥í•˜ì—¬ 'ë³€ë³„ë ¥' í™•ë³´
    min_match_len = 6
    
    # ê²¹ì¹˜ëŠ” êµ¬ê°„ ì°¾ê¸°
    to_highlight = []
    for i in range(len(target) - min_match_len + 1):
        chunk = target[i:i+min_match_len]
        if " " in chunk and len(chunk.strip()) < min_match_len: continue # ê³µë°± ì œì™¸ ì‹¤ì§ˆ ê¸€ììˆ˜ ì²´í¬
        
        chunk_stripped = re.sub(r'\s+', '', chunk)
        if chunk_stripped in ref_stripped:
            to_highlight.append(chunk)

    # ê¸´ ë¬¸êµ¬ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ë§ˆí‚¹ (ì¤‘ë³µ ë°©ì§€)
    sorted_chunks = sorted(list(set(to_highlight)), key=len, reverse=True)
    
    result = target
    for chunk in sorted_chunks:
        # ì´ë¯¸ mark íƒœê·¸ê°€ ì ìš©ëœ ë¶€ë¶„ì€ ê±´ë“œë¦¬ì§€ ì•Šë„ë¡ ë³´í˜¸
        if chunk in result:
            result = result.replace(chunk, f"[[MARK_START]]{chunk}[[MARK_END]]")
    
    result = result.replace("[[MARK_START]]", "<mark>").replace("[[MARK_END]]", "</mark>")
    # ì—°ì†ëœ mark íƒœê·¸ ë³‘í•©
    result = re.sub(r'</mark><mark>', '', result)
    return result

# --- UI ë° ë¶„ì„ ë¡œì§ ---
st.title("ğŸ” ë¬¸í•­ ìœ ì‚¬ë„ ì •ë°€ ë¶„ì„ê¸°")

col1, col2 = st.columns(2)
with col1:
    file_origin = st.file_uploader("ğŸ“˜
